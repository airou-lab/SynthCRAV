{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "qc2aBWFz-V5a",
      "metadata": {
        "id": "qc2aBWFz-V5a"
      },
      "source": [
        "# SynthCRAV - Synthetic noise simulation and recognition for Camera Radar autonomous vehicles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2df8fbb",
      "metadata": {},
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b694c212",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e436b420",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import struct\n",
        "from copy import copy\n",
        "\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers.wandb import WandbLogger\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import torchviz\n",
        "import wandb\n",
        "\n",
        "from nuscenes import NuScenes\n",
        "from nuscenes.utils import splits\n",
        "\n",
        "# from models.models_utils.utils import *\n",
        "# from models.models_utils.config import device, ndevice\n",
        "# from models.models import RadarNDet, CameraNDet\n",
        "\n",
        "sensor_list = ['CAM_BACK','CAM_BACK_LEFT','CAM_BACK_RIGHT','CAM_FRONT','CAM_FRONT_LEFT','CAM_FRONT_RIGHT',\n",
        "                'RADAR_FRONT','RADAR_FRONT_LEFT','RADAR_FRONT_RIGHT','RADAR_BACK_LEFT','RADAR_BACK_RIGHT']\n",
        "\n",
        "cam_list = ['CAM_BACK','CAM_BACK_LEFT','CAM_BACK_RIGHT','CAM_FRONT','CAM_FRONT_LEFT','CAM_FRONT_RIGHT']\n",
        "\n",
        "radar_list = ['RADAR_FRONT','RADAR_FRONT_LEFT','RADAR_FRONT_RIGHT','RADAR_BACK_LEFT','RADAR_BACK_RIGHT']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8f550fe1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "found CUDA device: NVIDIA GeForce RTX 3060\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    ndevice = torch.cuda.current_device()\n",
        "    # if os.getpid() == 1:  # Not working on windows distro\n",
        "    #     print('\\nfound CUDA device:', torch.cuda.get_device_name(ndevice))\n",
        "    print('\\nfound CUDA device:', torch.cuda.get_device_name(ndevice))\n",
        "else:\n",
        "    # if os.getpid() == 1: # Not working on windows distro\n",
        "    #     print('\\nno CUDA installation found, using CPU')\n",
        "    print('\\nno CUDA installation found, using CPU')\n",
        "    device = torch.device('cpu')\n",
        "    ndevice = torch.cuda.current_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e4515e0",
      "metadata": {},
      "source": [
        "### Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3c2105c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# UTILS\n",
        "def decode_pcd_file(filename,verbose=False):\n",
        "    # Extract sensor data\n",
        "    if verbose:\n",
        "        print('Opening point cloud data at:', filename)\n",
        "\n",
        "    # opening file    \n",
        "    meta = []\n",
        "    with open (filename, 'rb') as file:\n",
        "        for line in file:\n",
        "            line = line.strip().decode('utf-8')\n",
        "            meta.append(line)                        \n",
        "\n",
        "            if line.startswith('DATA'):\n",
        "                break\n",
        "\n",
        "        data_binary = file.read()\n",
        "\n",
        "    #extracting headers\n",
        "    fields = meta[2].split(' ')[1:]\n",
        "    sizes = meta[3].split(' ')[1:]\n",
        "    types = meta[4].split(' ')[1:]\n",
        "    width = int(meta[6].split(' ')[1])\n",
        "    height = int(meta[7].split(' ')[1])\n",
        "    data = meta[10].split(' ')[1]\n",
        "    feature_count = len(types)                    \n",
        "    \n",
        "    unpacking_lut = {'F': {2: 'e', 4: 'f', 8: 'd'},\n",
        "             'I': {1: 'b', 2: 'h', 4: 'i', 8: 'q'},\n",
        "             'U': {1: 'B', 2: 'H', 4: 'I', 8: 'Q'}}\n",
        "    types_str = ''.join([unpacking_lut[t][int(s)] for t, s in zip(types, sizes)])\n",
        "\n",
        "    # Decode each point\n",
        "    offset = 0\n",
        "    point_count = width\n",
        "    points = []\n",
        "    for i in range(point_count):\n",
        "        point = []\n",
        "        for p in range(feature_count):\n",
        "            start_p = offset\n",
        "            end_p = start_p + int(sizes[p])\n",
        "            assert end_p < len(data_binary)\n",
        "            point_p = struct.unpack(types_str[p], data_binary[start_p:end_p])[0]\n",
        "            point.append(point_p)\n",
        "            offset = end_p\n",
        "        points.append(point)\n",
        "\n",
        "    # store in dataframe\n",
        "    df = pd.DataFrame(points,columns=fields, dtype=object)\n",
        "\n",
        "    return df, types_str\n",
        "\n",
        "def convert_radardf_to_tensor(radar_df, types_str):\n",
        "    npdtype_list = []\n",
        "    tensor_list = []\n",
        "    torchdtype = torch.float32\n",
        "\n",
        "    for typechar in types_str:\n",
        "        # floats\n",
        "        if typechar == 'e':\n",
        "            npdtype = np.float16\n",
        "\n",
        "        elif typechar == 'f':\n",
        "            npdtype = np.float32\n",
        "\n",
        "        elif typechar == 'd':\n",
        "            npdtype = np.float64\n",
        "            torchdtype = torch.float64 # promote to 64 floats if at least one column is in this type\n",
        "\n",
        "        # signed int\n",
        "        elif typechar == 'b':\n",
        "            npdtype = np.int8\n",
        "\n",
        "        elif typechar == 'h':\n",
        "            npdtype = np.int16\n",
        "\n",
        "        elif typechar == 'i':\n",
        "            npdtype = np.int32\n",
        "\n",
        "        elif typechar == 'q':\n",
        "            npdtype = np.int64\n",
        "\n",
        "        # unsigned int\n",
        "        elif typechar == 'B':\n",
        "            npdtype = np.uint8\n",
        "\n",
        "        elif typechar == 'H':\n",
        "            npdtype = np.uint16\n",
        "\n",
        "        elif typechar == 'I':\n",
        "            npdtype = np.uint32\n",
        "\n",
        "        elif typechar == 'Q':\n",
        "            npdtype = np.uint64\n",
        "\n",
        "        npdtype_list.append(npdtype)\n",
        "\n",
        "    for col, dtypenp in zip(radar_df.columns,npdtype_list):\n",
        "        tensor_list.append(torch.tensor(radar_df[col].values.astype(dtypenp), dtype=torch.float32))\n",
        "        \n",
        "    combined_tensor = torch.stack(tensor_list, dim=-1)\n",
        "\n",
        "    return combined_tensor\n",
        "\n",
        "def get_labels(data_module):\n",
        "    # Loading labels\n",
        "    labels = copy(data_module.df_train['labels']).drop_duplicates().sort_values().reset_index(drop=True)\n",
        "    n_labels = len(labels)\n",
        "\n",
        "    return labels, len(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8cbc25",
      "metadata": {},
      "source": [
        "### Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e11f0808",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset loading utils\n",
        "def get_df_split(nusc, args, sensor_type, data_split):\n",
        "    '''\n",
        "    For Cameras the results are stored in noisy_nuScenes/samples/sensor/<noise_level>/<noise_type>/<name.jpg>\n",
        "    For Radars the results are stored in noisy_nuScenes/samples/sensor/<noise_level>/<name.pcd>\n",
        "    '''\n",
        "    # output\n",
        "    data_paths = []\n",
        "    labels = []\n",
        "    sensors_list = []\n",
        "\n",
        "    # accumulate in df:\n",
        "    for scene in nusc.scene:\n",
        "        if scene['name'] not in data_split:\n",
        "            continue\n",
        "\n",
        "        nusc_sample = nusc.get('sample', scene['first_sample_token'])\n",
        "\n",
        "        while True:\n",
        "            if sensor_type == 'CAM':\n",
        "                for sensor in cam_list:\n",
        "                    # Load nusc info\n",
        "                    sample_data = nusc.get('sample_data', nusc_sample['data'][sensor])\n",
        "                    filename = sample_data['filename']\n",
        "                    token = filename.split('/')[-1]\n",
        "\n",
        "                    getOG=False\n",
        "\n",
        "                    for noise_level in range (10,110,10):\n",
        "                        for noise_type in ['Blur', 'Gaussian_noise', 'High_exposure', 'Low_exposure']:\n",
        "                            synthpath = os.path.join(args.data_root,'samples',sensor,str(noise_level),noise_type,token)\n",
        "                            if os.path.exists(synthpath):\n",
        "                                data_paths.append(synthpath)\n",
        "                                labels.append(int(noise_level/10))\n",
        "                                sensors_list.append(sensor)\n",
        "                                getOG=True # signal flag that data is good to take from OG as well\n",
        "\n",
        "                if getOG:\n",
        "                    data_paths.append(os.path.join(args.nusc_root,filename))\n",
        "                    labels.append(0)\n",
        "                    sensors_list.append(sensor)\n",
        "\n",
        "            elif sensor_type == 'RADAR':\n",
        "                for sensor in radar_list:\n",
        "                    # Load nusc info\n",
        "                    sample_data = nusc.get('sample_data', nusc_sample['data'][sensor])\n",
        "                    filename = sample_data['filename']\n",
        "                    token = filename.split('/')[-1]\n",
        "\n",
        "                    getOG=False\n",
        "\n",
        "                    for noise_level in range (10,110,10):\n",
        "                        synthpath = os.path.join(args.data_root,'samples',sensor,str(noise_level),token)\n",
        "                        if os.path.exists(synthpath):\n",
        "                            # removing empty dataframes\n",
        "                            radar_df, types_str = decode_pcd_file(synthpath,verbose=False)\n",
        "                            if not radar_df.isnull().values.any():\n",
        "                                data_paths.append(synthpath)\n",
        "                                labels.append(int(noise_level/10))\n",
        "                                sensors_list.append(sensor)\n",
        "                            getOG=True # signal flag that data is good to take from OG as well\n",
        "\n",
        "\n",
        "                    if getOG:\n",
        "                        # removing empty dataframes\n",
        "                        radar_df, types_str = decode_pcd_file(synthpath,verbose=False)\n",
        "                        if not radar_df.isnull().values.any():\n",
        "                            data_paths.append(os.path.join(args.nusc_root,filename))\n",
        "                            labels.append(0)\n",
        "                            sensors_list.append(sensor)\n",
        "\n",
        "\n",
        "            if nusc_sample['next'] == \"\":\n",
        "                #GOTO next scene\n",
        "                break\n",
        "            else:\n",
        "                #GOTO next sample\n",
        "                next_token = nusc_sample['next']\n",
        "                nusc_sample = nusc.get('sample', next_token)\n",
        "    \n",
        "    df = pd.DataFrame({'data':data_paths,'labels':labels,'sensor':sensors_list})\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_df(args, nusc, sensor):\n",
        "    # accumulate scene names\n",
        "    trainval = splits.mini_train\n",
        "    test_split = splits.mini_val\n",
        "\n",
        "    if sensor == 'CAM':\n",
        "        trainval = trainval[:-3]  # removing night scenes for camera noise\n",
        "\n",
        "    n_train_scenes = int(len(trainval)*args.ntrain)\n",
        "    n_val_scenes = len(trainval) - n_train_scenes\n",
        "\n",
        "\n",
        "    train_split, val_split = random_split(trainval,[n_train_scenes,n_val_scenes],generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "    if args.smaller_dataset:\n",
        "        # Smaller dataset with only 1 scenes for train/val/test, totalling 3 scenes. Required for smaller configs\n",
        "        train_split = random.choice(list(train_split))\n",
        "        val_split = random.choice(list(val_split))\n",
        "        test_split = random.choice(list(test_split))\n",
        "    else:\n",
        "        # using the full dataset, requires high-end computer\n",
        "        train_split = list(train_split)\n",
        "        val_split = list(val_split)\n",
        "        test_split = list(test_split)\n",
        "\n",
        "    # Generating output splits\n",
        "    n_train_scenes = 1\n",
        "    n_val_scenes = 1\n",
        "    df_train = get_df_split(nusc, args, sensor, train_split)\n",
        "    df_val   = get_df_split(nusc, args, sensor, val_split)\n",
        "    df_test  = get_df_split(nusc, args, sensor, test_split)\n",
        "\n",
        "    if args.verbose:\n",
        "        print('sensor:',sensor)\n",
        "        print('trainval:',trainval)\n",
        "        print('n_train_scenes:',n_train_scenes)\n",
        "        print('n_val_scenes:',n_val_scenes)\n",
        "        \n",
        "        print('train_split:',train_split)\n",
        "        print('val_split:',val_split)\n",
        "        print('test_split:',test_split)\n",
        "\n",
        "        print('train dataset:',df_train)\n",
        "        print('test dataset:',df_val)\n",
        "        print('val dataset:',df_test)\n",
        "\n",
        "    return df_train, df_val, df_test  \n",
        "\n",
        "def load_pcd(row):\n",
        "    '''\n",
        "    takes a df in input (batch)\n",
        "    returns a tensor with the loaded images\n",
        "    '''\n",
        "    labels = torch.tensor(row['labels'], dtype=torch.long)\n",
        "    radar_df, types_str = decode_pcd_file(row['data'],verbose=False)\n",
        "\n",
        "    data = convert_radardf_to_tensor(radar_df,types_str)\n",
        "    # print (data.shape)\n",
        "    # data = data.unsqueeze(0).transpose(1, 2)  # Shape: [1, N, C] -> [1, C, N]\n",
        "    data = data.transpose(0, 1)  # Shape: [N, C] -> [C, N]\n",
        "\n",
        "    return data, labels, radar_df\n",
        "\n",
        "def load_pcd_masked(row):\n",
        "    '''\n",
        "    takes a df in input (batch)\n",
        "    returns a tensor with the loaded images\n",
        "    '''\n",
        "    N=256\n",
        "    labels = torch.tensor(row['labels'], dtype=torch.long)\n",
        "    radar_df, types_str = decode_pcd_file(row['data'],verbose=False)\n",
        "\n",
        "    data = convert_radardf_to_tensor(radar_df,types_str)\n",
        "\n",
        "    n_pads = N - data.shape[0]\n",
        "\n",
        "    if n_pads>0:\n",
        "        pad = torch.zeros((n_pads, data.shape[1]), dtype=data.dtype, device=data.device)\n",
        "        padded_data = torch.cat([data, pad], dim=0)\n",
        "    else:\n",
        "        padded_data = data[:N]\n",
        "\n",
        "\n",
        "    # print('data.shape:',data.shape)\n",
        "    # data = data.unsqueeze(0).transpose(1, 2)  # Shape: [1, N, C] -> [1, C, N]\n",
        "    padded_data = padded_data.transpose(0, 1)  # Shape: [N, C] -> [C, N]\n",
        "\n",
        "    # print('padded_data:',padded_data)\n",
        "    # print('padded_data.shape:',padded_data.shape)\n",
        "    # input()\n",
        "\n",
        "    # Creating a mask to ignore padded points in training\n",
        "    mask = torch.zeros(N, dtype=torch.bool)\n",
        "    mask[:min(data.shape[0], N)] = True\n",
        "\n",
        "    return padded_data, labels, mask\n",
        "\n",
        "def load_imgs(row):\n",
        "    '''\n",
        "    takes a row of a df in input\n",
        "    returns a tensor with the loaded image and a tensor of its label\n",
        "    '''\n",
        "    img = cv2.imread(row['data'])\n",
        "\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image not found at {row['data']}\")\n",
        "\n",
        "    data = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
        "    labels = torch.tensor(row['labels'], dtype=torch.long)\n",
        "    return data, labels, img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7fe2d3ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DATALOADER\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        data, labels, _ =load_imgs(row)\n",
        "        return data, labels\n",
        "\n",
        "class RadarDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        # data, labels, _ =load_pcd(row)\n",
        "        data, labels, mask =load_pcd_masked(row)\n",
        "        if not data.shape[1]:  # Skip empty point clouds\n",
        "            return None\n",
        "        return data, labels, mask\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        xs, ys, dfs, masks = zip(*batch)\n",
        "        xs = torch.stack(xs)\n",
        "        ys = torch.stack(ys)\n",
        "        masks = torch.stack(masks)\n",
        "        return xs, ys, masks\n",
        "\n",
        "\n",
        "class DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, args, sensor, batch_size=16, n_workers=0):\n",
        "        super().__init__()\n",
        "\n",
        "        nusc = NuScenes(version='v1.0-mini', dataroot=args.nusc_root, verbose=True)# loading nusc table\n",
        "        self.df_train, self.df_val, self.df_test = create_df(args, nusc, sensor)\n",
        "        self.batch_size = batch_size\n",
        "        self.sensor = sensor\n",
        "        self.n_workers = n_workers\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if self.sensor == 'CAM':\n",
        "            train_dataset = ImageDataset(self.df_train)\n",
        "        elif self.sensor == 'RADAR':\n",
        "            train_dataset = RadarDataset(self.df_train)\n",
        "\n",
        "        if self.n_workers:\n",
        "            return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.n_workers, persistent_workers=True,  pin_memory=True)\n",
        "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        if self.sensor == 'CAM':\n",
        "            val_dataset = ImageDataset(self.df_val)\n",
        "        elif self.sensor == 'RADAR':\n",
        "            val_dataset = RadarDataset(self.df_val)\n",
        "        if self.n_workers:\n",
        "            return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.n_workers, persistent_workers=True,  pin_memory=True)\n",
        "        return DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        if self.sensor == 'CAM':\n",
        "            test_dataset = ImageDataset(self.df_test)\n",
        "        elif self.sensor == 'RADAR':\n",
        "            test_dataset = RadarDataset(self.df_test)\n",
        "        if self.n_workers:\n",
        "            return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.n_workers, persistent_workers=True,  pin_memory=True)\n",
        "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d3f4342",
      "metadata": {},
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f74d7334",
      "metadata": {},
      "source": [
        "### Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4bd9b572",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization utils\n",
        "# Neural Net plots\n",
        "def plot_trainval_loss(hist_filename):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "\n",
        "    # load hist\n",
        "    with open(hist_filename, \"rb\") as f:\n",
        "        hist = pickle.load(f) \n",
        "\n",
        "    train_loss = hist['train_loss']\n",
        "    val_loss = hist['val_loss']\n",
        "    n_epochs = len(train_loss)\n",
        "\n",
        "    print()\n",
        "    print('n_epochs:',n_epochs)\n",
        "    print('train_loss:',train_loss)\n",
        "    print('val_loss:',val_loss)\n",
        "\n",
        "    plt.plot(range(n_epochs),train_loss)\n",
        "    plt.plot(range(n_epochs),val_loss)\n",
        "\n",
        "    plt.title('Train/Val loss vs Epochs')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "\n",
        "    plt.legend(['Train loss','Val loss'])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def plot_trainval_acc(hist_filename):\n",
        "    plt.figure(figsize=(16, 9))\n",
        "\n",
        "    # load hist\n",
        "    with open(hist_filename, \"rb\") as f:\n",
        "        hist = pickle.load(f) \n",
        "\n",
        "    train_acc = hist['train_accuracy']\n",
        "    val_acc = hist['val_accuracy']\n",
        "    n_epochs = len(train_acc)\n",
        "\n",
        "    print()\n",
        "    print('n_epochs:',n_epochs)\n",
        "    print('train_acc:',train_acc)\n",
        "    print('val_acc:',val_acc)\n",
        "\n",
        "    plt.plot(range(n_epochs),train_acc)\n",
        "    plt.plot(range(n_epochs),val_acc)\n",
        "\n",
        "    plt.title('Train/Val accuracy vs Epochs')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.legend(['Train accuracy','Val accuracy'])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_test_acc(hist_filename):\n",
        "    # load hist\n",
        "    with open(hist_filename, \"rb\") as f:\n",
        "        hist = pickle.load(f) \n",
        "\n",
        "    test_loss = hist['test_loss']\n",
        "    test_accuracy = hist['test_accuracy']\n",
        "\n",
        "    print()\n",
        "    print('test_loss:',test_loss)\n",
        "    print('test_accuracy:',test_accuracy)\n",
        "\n",
        "def get_TP_FP(hist_filename):\n",
        "    with open(hist_filename, \"rb\") as f:\n",
        "        hist = pickle.load(f)\n",
        "\n",
        "    preds = hist['test_results']['preds']\n",
        "    labels = hist['test_results']['labels']\n",
        "    n_classes = 11\n",
        "    TP = np.zeros(n_classes, dtype=int)\n",
        "    FN = np.zeros(n_classes, dtype=int)\n",
        "\n",
        "    for c in range(n_classes):\n",
        "        TP[c] = np.sum((preds == c) & (labels == c))  # Correctly predicted class c\n",
        "        FN[c] = np.sum((preds != c) & (labels == c))  # Missed class c\n",
        "\n",
        "    print('TP:',TP,sum(TP))\n",
        "    print('FN:',FN,sum(FN))\n",
        "\n",
        "def plot_confusion_mat(y_true, y_pred, name):\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    print('TP:',np.trace(cm))\n",
        "    print('FN:',len(y_true)-np.trace(cm))\n",
        "\n",
        "    num_labels = np.arange(0,11,dtype=int).tolist()\n",
        "    label_list = np.array([0,10,20,30,40,50,60,70,80,90,100])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16,9))\n",
        "    disp = ConfusionMatrixDisplay.from_predictions(y_true=y_true, y_pred=y_pred, labels=num_labels, display_labels=label_list, ax=ax, colorbar=False)\n",
        "    disp.plot(ax=ax,cmap=plt.cm.Blues, xticks_rotation=45)\n",
        "    plt.savefig(name,dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea780af",
      "metadata": {},
      "source": [
        "### Backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "86c4581d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class legacyradar1(pl.LightningModule):\n",
        "    def __init__(self, n_channels, output_size, history, dropout_prob=0, lr=1e-3):\n",
        "        super(legacyradar1, self).__init__()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.loss_fct=nn.CrossEntropyLoss()\n",
        "        self.train_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.validation_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.test_step_outputs = {'acc':[],'loss':[],'preds':[],'labels':[]}\n",
        "        self.history = history\n",
        "\n",
        "        # ------------------------------------------model------------------------------------------\n",
        "        self.input = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=32, kernel_size=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob))\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=32, out_channels=64, kernel_size=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob))\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob))\n",
        "\n",
        "        # self.conv3 = nn.Sequential(nn.Conv1d(in_channels=128, out_channels=256, kernel_size=1),\n",
        "        #                             nn.ReLU(),\n",
        "        #                             nn.Dropout1d(p=dropout_prob))\n",
        "\n",
        "        \n",
        "        self.fc1 = nn.Sequential(nn.Linear(128,64),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(p=dropout_prob))\n",
        "\n",
        "        self.fc2 = nn.Sequential(nn.Linear(64,32),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout(p=dropout_prob))\n",
        "\n",
        "        self.head = nn.Sequential(nn.Linear(32,output_size))\n",
        "        # no softmax because we use cross entropy loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # x = self.conv3(x)\n",
        "\n",
        "        x = torch.mean(x, dim=2)  # global average pooling\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr, eps=1e-20,weight_decay=1e-5)\n",
        "\n",
        "\n",
        "\n",
        "    # train \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1) \n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "\n",
        "        self.log('train_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('train_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('train_acc', acc, prog_bar=True) \n",
        "        # self.log('train_acc_epoch', acc, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        self.train_step_outputs['loss'].append(loss.item())\n",
        "        self.train_step_outputs['acc'].append(acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.train_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.train_step_outputs['loss'])\n",
        "        self.log('train_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['train_loss'].append(tot_loss)\n",
        "        self.history['train_accuracy'].append(avg_acc)\n",
        "\n",
        "        self.train_step_outputs['acc']=[]  # clear for next epoch\n",
        "        self.train_step_outputs['loss']=[]  # clear for next epoch\n",
        "\n",
        "        wandb.log({'loss': tot_loss, 'accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "\n",
        "    # val\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        if not self.trainer.sanity_checking:\n",
        "            self.log('val_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "            self.log('val_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "            # self.log('val_loss', loss, prog_bar=True)\n",
        "            # self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "            self.validation_step_outputs['loss'].append(loss)\n",
        "            self.validation_step_outputs['acc'].append(acc)\n",
        "        \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.trainer.sanity_checking:\n",
        "            avg_acc = torch.stack(self.validation_step_outputs['acc']).mean()\n",
        "            tot_loss = sum(self.validation_step_outputs['loss'])\n",
        "            self.log('val_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "            self.history['val_loss'].append(tot_loss)\n",
        "            self.history['val_accuracy'].append(avg_acc)\n",
        "\n",
        "            self.validation_step_outputs['acc']=[]  # clear for next epoch\n",
        "            self.validation_step_outputs['loss']=[]  # clear for next epoch\n",
        "        \n",
        "            wandb.log({'val_loss': tot_loss, 'val_accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        self.log('test_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('test_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "        self.test_step_outputs['loss'].append(loss.item())\n",
        "        self.test_step_outputs['acc'].append(acc)\n",
        "        self.test_step_outputs['preds'].append(preds.cpu())\n",
        "        self.test_step_outputs['labels'].append(y.cpu())\n",
        "\n",
        "        return loss    \n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.test_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.test_step_outputs['loss'])\n",
        "        self.log('test_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['test_loss'].append(tot_loss)\n",
        "        self.history['test_accuracy'].append(avg_acc)\n",
        "\n",
        "        all_preds = torch.cat(self.test_step_outputs['preds']).numpy()\n",
        "        all_labels = torch.cat(self.test_step_outputs['labels']).numpy()\n",
        "\n",
        "        plot_confusion_mat(all_preds, all_labels, os.path.join('./ckpt','radar_model_mat.png'))\n",
        "\n",
        "        wandb.log({'test_loss': tot_loss, 'test_accuracy': avg_acc}, step=self.current_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "406bc23d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Camera noise classifier\n",
        "class legacycam2(pl.LightningModule):\n",
        "    def __init__(self, image_shape, output_size, history, conv_k=3, dropout_prob=0, lr=1e-3):\n",
        "        super(legacycam2, self).__init__()\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.loss_fct=nn.CrossEntropyLoss()\n",
        "        self.train_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.validation_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.test_step_outputs = {'acc':[],'loss':[],'preds':[],'labels':[]}\n",
        "        self.history = history\n",
        "\n",
        "        image_size = np.array(image_shape[:2]) # 1080 x 1920\n",
        "        in_channels = image_shape[2] # 3\n",
        "\n",
        "        # ------------------------------------------model------------------------------------------\n",
        "        self.input = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size=np.floor((image_size-2)/2) \n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size=np.floor((image_size-2)/2) \n",
        "        \n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size = np.floor((image_size-2)/2) \n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size = np.floor((image_size-2)/2) \n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size = np.floor((image_size-2)/2) #\n",
        "\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size = np.floor((image_size-2)/2) #\n",
        "\n",
        "\n",
        "        self.conv6 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        image_size = np.floor((image_size-2)/2) #\n",
        "\n",
        "        self.head = nn.Sequential(nn.Flatten(),\n",
        "                                  nn.Linear(int(8*image_size[0]*image_size[1]),output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        \n",
        "        x = self.head(x)\n",
        "        return x\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr, eps=1e-20,weight_decay=1e-5)\n",
        "\n",
        "\n",
        "\n",
        "    # train \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1) \n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "\n",
        "        self.log('train_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('train_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('train_acc', acc, prog_bar=True) \n",
        "        # self.log('train_acc_epoch', acc, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        self.train_step_outputs['loss'].append(loss.item())\n",
        "        self.train_step_outputs['acc'].append(acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.train_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.train_step_outputs['loss'])\n",
        "        self.log('train_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['train_loss'].append(tot_loss)\n",
        "        self.history['train_accuracy'].append(avg_acc)\n",
        "\n",
        "        self.train_step_outputs['acc']=[]  # clear for next epoch\n",
        "        self.train_step_outputs['loss']=[]  # clear for next epoch\n",
        "\n",
        "        wandb.log({'loss': tot_loss, 'accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "\n",
        "    # val\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        if not self.trainer.sanity_checking:\n",
        "            self.log('val_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "            self.log('val_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "            # self.log('val_loss', loss, prog_bar=True)\n",
        "            # self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "            self.validation_step_outputs['loss'].append(loss)\n",
        "            self.validation_step_outputs['acc'].append(acc)\n",
        "        \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.trainer.sanity_checking:\n",
        "            avg_acc = torch.stack(self.validation_step_outputs['acc']).mean()\n",
        "            tot_loss = sum(self.validation_step_outputs['loss'])\n",
        "            self.log('val_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "            self.history['val_loss'].append(tot_loss)\n",
        "            self.history['val_accuracy'].append(avg_acc)\n",
        "\n",
        "            self.validation_step_outputs['acc']=[]  # clear for next epoch\n",
        "            self.validation_step_outputs['loss']=[]  # clear for next epoch\n",
        "        \n",
        "            wandb.log({'val_loss': tot_loss, 'val_accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        self.log('test_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('test_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "        self.test_step_outputs['loss'].append(loss.item())\n",
        "        self.test_step_outputs['acc'].append(acc)\n",
        "        self.test_step_outputs['preds'].append(preds.cpu())\n",
        "        self.test_step_outputs['labels'].append(y.cpu())\n",
        "\n",
        "        return loss    \n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.test_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.test_step_outputs['loss'])\n",
        "        self.log('test_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['test_loss'].append(tot_loss)\n",
        "        self.history['test_accuracy'].append(avg_acc)\n",
        "\n",
        "        all_preds = torch.cat(self.test_step_outputs['preds']).numpy()\n",
        "        all_labels = torch.cat(self.test_step_outputs['labels']).numpy()\n",
        "\n",
        "        plot_confusion_mat(all_preds, all_labels, os.path.join('./ckpt','radar_model_mat.png'))\n",
        "\n",
        "        wandb.log({'test_loss': tot_loss, 'test_accuracy': avg_acc}, step=self.current_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880c7176",
      "metadata": {},
      "source": [
        "### Radar "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f43edd69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Radar noise classifier\n",
        "class RadarNDet(pl.LightningModule):\n",
        "    def __init__(self, n_channels, output_size, history, dropout_prob=0, lr=1e-3):\n",
        "        super(RadarNDet, self).__init__()\n",
        "\n",
        "        self.lr = lr\n",
        "        self.loss_fct=nn.CrossEntropyLoss()\n",
        "        self.train_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.validation_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.test_step_outputs = {'acc':[],'loss':[],'preds':[],'labels':[]}\n",
        "        self.history = history\n",
        "\n",
        "        # ------------------------------------------model------------------------------------------\n",
        "        # self.input =  nn.Linear(input_size, embed_size)\n",
        "        self.input = nn.Sequential(nn.Conv1d(in_channels=n_channels, out_channels=32, kernel_size=3, padding=1),\n",
        "                                    nn.BatchNorm1d(32),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob),\n",
        "                                    )\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "                                    nn.BatchNorm1d(64),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob),\n",
        "                                    )\n",
        "\n",
        "        self.fc1 =  nn.Sequential(nn.Linear(64, 32),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout1d(p=dropout_prob)\n",
        "                                    )\n",
        "\n",
        "        self.attention1 = nn.MultiheadAttention(32, num_heads=1, batch_first=True)\n",
        "\n",
        "        self.head = nn.Sequential(nn.Linear(32, output_size))\n",
        "        # no softmax because we use cross entropy loss\n",
        "\n",
        "    def forward(self, x,mask=None):\n",
        "        x = self.input(x)\n",
        "        x = self.conv1(x)\n",
        "        x = x.permute(0, 2, 1)  # (batch_size, seq_len, features)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask should be (B, N), False = padding, True = keep\n",
        "            x, _ = self.attention1(x, x, x, key_padding_mask=~mask )\n",
        "        else:\n",
        "            x, _ = self.attention1(x, x, x)\n",
        "\n",
        "        # x, _ = self.attention1(x, x, x)\n",
        "        last_hidden = x[:, -1, :]\n",
        "        x = self.head(last_hidden)\n",
        "\n",
        "        return x\n",
        "    \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr, eps=1e-20,weight_decay=1e-5)\n",
        "\n",
        "\n",
        "\n",
        "    # train \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1) \n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "\n",
        "        self.log('train_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('train_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('train_acc', acc, prog_bar=True) \n",
        "        # self.log('train_acc_epoch', acc, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        self.train_step_outputs['loss'].append(loss.item())\n",
        "        self.train_step_outputs['acc'].append(acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.train_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.train_step_outputs['loss'])\n",
        "        self.log('train_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['train_loss'].append(tot_loss)\n",
        "        self.history['train_accuracy'].append(avg_acc)\n",
        "\n",
        "        self.train_step_outputs['acc']=[]  # clear for next epoch\n",
        "        self.train_step_outputs['loss']=[]  # clear for next epoch\n",
        "\n",
        "        wandb.log({'loss': tot_loss, 'accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "\n",
        "    # val\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        if not self.trainer.sanity_checking:\n",
        "            self.log('val_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "            self.log('val_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "            # self.log('val_loss', loss, prog_bar=True)\n",
        "            # self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "            self.validation_step_outputs['loss'].append(loss)\n",
        "            self.validation_step_outputs['acc'].append(acc)\n",
        "        \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.trainer.sanity_checking:\n",
        "            avg_acc = torch.stack(self.validation_step_outputs['acc']).mean()\n",
        "            tot_loss = sum(self.validation_step_outputs['loss'])\n",
        "            self.log('val_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "            self.history['val_loss'].append(tot_loss)\n",
        "            self.history['val_accuracy'].append(avg_acc)\n",
        "\n",
        "            self.validation_step_outputs['acc']=[]  # clear for next epoch\n",
        "            self.validation_step_outputs['loss']=[]  # clear for next epoch\n",
        "        \n",
        "            wandb.log({'val_loss': tot_loss, 'val_accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y, mask = batch\n",
        "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
        "        y_hat = self(x, mask=mask)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        self.log('test_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('test_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "        self.test_step_outputs['loss'].append(loss.item())\n",
        "        self.test_step_outputs['acc'].append(acc)\n",
        "        self.test_step_outputs['preds'].append(preds.cpu())\n",
        "        self.test_step_outputs['labels'].append(y.cpu())\n",
        "\n",
        "        return loss    \n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.test_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.test_step_outputs['loss'])\n",
        "        self.log('test_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['test_loss'].append(tot_loss)\n",
        "        self.history['test_accuracy'].append(avg_acc)\n",
        "\n",
        "        all_preds = torch.cat(self.test_step_outputs['preds']).numpy()\n",
        "        all_labels = torch.cat(self.test_step_outputs['labels']).numpy()\n",
        "\n",
        "        plot_confusion_mat(all_preds, all_labels, os.path.join('./ckpt','radar_model_mat.png'))\n",
        "\n",
        "        wandb.log({'test_loss': tot_loss, 'test_accuracy': avg_acc}, step=self.current_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd221f85",
      "metadata": {},
      "source": [
        "### Camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a1519160",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Camera noise classifier\n",
        "class CameraNDet(pl.LightningModule):\n",
        "    def __init__(self, image_shape, output_size, history, conv_k=3, dropout_prob=0, lr=1e-3):\n",
        "        super(CameraNDet, self).__init__()\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.loss_fct=nn.CrossEntropyLoss()\n",
        "        self.train_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.validation_step_outputs = {'acc':[],'loss':[]}\n",
        "        self.test_step_outputs = {'acc':[],'loss':[],'preds':[],'labels':[]}\n",
        "        self.history = history\n",
        "\n",
        "        image_size = np.array(image_shape[:2]) # 1080 x 1920\n",
        "        in_channels = image_shape[2] # 3\n",
        "\n",
        "        # ------------------------------------------model------------------------------------------\n",
        "        self.input = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "        \n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "\n",
        "\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(conv_k,conv_k), \\\n",
        "                               stride=1, padding=0, bias=True, padding_mode = 'zeros'),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Dropout2d(p=dropout_prob),\n",
        "                               nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(8, output_size)\n",
        "                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        \n",
        "        x = self.head(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr, eps=1e-20)\n",
        "\n",
        "\n",
        "\n",
        "    # train \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = self(x)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1) \n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "\n",
        "        self.log('train_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('train_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('train_acc', acc, prog_bar=True) \n",
        "        # self.log('train_acc_epoch', acc, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        self.train_step_outputs['loss'].append(loss.item())\n",
        "        self.train_step_outputs['acc'].append(acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.train_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.train_step_outputs['loss'])\n",
        "        self.log('train_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['train_loss'].append(tot_loss)\n",
        "        self.history['train_accuracy'].append(avg_acc)\n",
        "\n",
        "        self.train_step_outputs['acc']=[]  # clear for next epoch\n",
        "        self.train_step_outputs['loss']=[]  # clear for next epoch\n",
        "\n",
        "        wandb.log({'loss': tot_loss, 'accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "\n",
        "    # val\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = self(x)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        if not self.trainer.sanity_checking:\n",
        "            self.log('val_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "            self.log('val_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "            # self.log('val_loss', loss, prog_bar=True)\n",
        "            # self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "            self.validation_step_outputs['loss'].append(loss)\n",
        "            self.validation_step_outputs['acc'].append(acc)\n",
        "        \n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if not self.trainer.sanity_checking:\n",
        "            avg_acc = torch.stack(self.validation_step_outputs['acc']).mean()\n",
        "            tot_loss = sum(self.validation_step_outputs['loss'])\n",
        "            self.log('val_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "            self.history['val_loss'].append(tot_loss)\n",
        "            self.history['val_accuracy'].append(avg_acc)\n",
        "\n",
        "            self.validation_step_outputs['acc']=[]  # clear for next epoch\n",
        "            self.validation_step_outputs['loss']=[]  # clear for next epoch\n",
        "        \n",
        "            wandb.log({'val_loss': tot_loss, 'val_accuracy': avg_acc}, step=self.current_epoch)\n",
        "\n",
        "\n",
        "    # test\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_hat = self(x)\n",
        "\n",
        "        #compute loss         \n",
        "        loss = self.loss_fct(y_hat, y)  \n",
        "\n",
        "        # Compute accuracy\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "\n",
        "        self.log('test_loss', loss.item(), prog_bar=True, on_step=True, on_epoch=False)\n",
        "        self.log('test_loss_epoch', loss.item(), prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        # self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "        self.test_step_outputs['loss'].append(loss.item())\n",
        "        self.test_step_outputs['acc'].append(acc)\n",
        "        self.test_step_outputs['preds'].append(preds.cpu())\n",
        "        self.test_step_outputs['labels'].append(y.cpu())\n",
        "\n",
        "        return loss    \n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        avg_acc = torch.stack(self.test_step_outputs['acc']).mean()\n",
        "        tot_loss = sum(self.test_step_outputs['loss'])\n",
        "        self.log('test_acc', avg_acc, prog_bar=False)\n",
        "\n",
        "        self.history['test_loss'].append(tot_loss)\n",
        "        self.history['test_accuracy'].append(avg_acc)\n",
        "\n",
        "        all_preds = torch.cat(self.test_step_outputs['preds']).numpy()\n",
        "        all_labels = torch.cat(self.test_step_outputs['labels']).numpy()\n",
        "\n",
        "        plot_confusion_mat(all_preds, all_labels, os.path.join('./ckpt','camera_model_mat.png'))\n",
        "\n",
        "        wandb.log({'test_loss': tot_loss, 'test_accuracy': avg_acc}, step=self.current_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d7cfb3",
      "metadata": {},
      "source": [
        "## Train Test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1e6a55",
      "metadata": {},
      "source": [
        "### Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b92cbc9d",
      "metadata": {},
      "source": [
        "Note: these were retrofitted from my original main.py. <br>\n",
        "Since most of the functions rely on argparse arguments, and future work probably will as well, I kept them as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fd398f09",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=16, conv_k=3, data_root='./data/noisy_nuScenes/', dropout=0.1, load_checkpoint=False, lr=0.0001, n_epochs=10, n_workers=0, ntrain=0.7, nusc_root='./data/default_nuScenes/', output_path='./ckpt/', save_model=True, smaller_dataset=True, verbose=2)\n"
          ]
        }
      ],
      "source": [
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    # input / output\n",
        "    nusc_root =         './data/default_nuScenes/',\n",
        "    data_root =         './data/noisy_nuScenes/',\n",
        "    output_path =       './ckpt/',\n",
        "    \n",
        "    # Dataset parameters\n",
        "    smaller_dataset=    True,\n",
        "    ntrain =            0.7,\n",
        "\n",
        "    # misc\n",
        "    n_workers =         0,\n",
        "\n",
        "    # actions\n",
        "    load_checkpoint =   False,\n",
        "    save_model =        True,\n",
        "\n",
        "    # hyperparameters (redefined for each model)\n",
        "    lr =                1e-4,\n",
        "    n_epochs =          10,\n",
        "    batch_size =        16,\n",
        "\n",
        "    # network parameters (redefined for each model)\n",
        "    conv_k =            3,\n",
        "    dropout =         0.1,\n",
        "\n",
        "    # Verbosity level\n",
        "    verbose =           2\n",
        ")\n",
        "\n",
        "print(args)\n",
        "\n",
        "\n",
        "if args.n_workers>0:\n",
        "    mp.set_start_method('spawn', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a90d16",
      "metadata": {},
      "source": [
        "### Main\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a40cae4",
      "metadata": {},
      "source": [
        "#### Camera\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "18ee1ca8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmathis-morales\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/mathis/Documents/SynthCRAV/wandb/run-20250508_174426-7j5577wa</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mathis-morales/CameraNR/runs/7j5577wa' target=\"_blank\">icy-monkey-26</a></strong> to <a href='https://wandb.ai/mathis-morales/CameraNR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mathis-morales/CameraNR' target=\"_blank\">https://wandb.ai/mathis-morales/CameraNR</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mathis-morales/CameraNR/runs/7j5577wa' target=\"_blank\">https://wandb.ai/mathis-morales/CameraNR/runs/7j5577wa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.416 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.0 seconds.\n",
            "======\n",
            "sensor: CAM\n",
            "trainval: ['scene-0061', 'scene-0553', 'scene-0655', 'scene-0757', 'scene-0796']\n",
            "n_train_scenes: 1\n",
            "n_val_scenes: 1\n",
            "train_split: scene-0796\n",
            "val_split: scene-0061\n",
            "test_split: scene-0103\n",
            "train dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9635  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9636  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9637  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9638  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9639  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9635  CAM_FRONT_RIGHT  \n",
            "9636  CAM_FRONT_RIGHT  \n",
            "9637  CAM_FRONT_RIGHT  \n",
            "9638  CAM_FRONT_RIGHT  \n",
            "9639  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9640 rows x 3 columns]\n",
            "test dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9394  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9395  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9396  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9397  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9398  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9394  CAM_FRONT_RIGHT  \n",
            "9395  CAM_FRONT_RIGHT  \n",
            "9396  CAM_FRONT_RIGHT  \n",
            "9397  CAM_FRONT_RIGHT  \n",
            "9398  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9399 rows x 3 columns]\n",
            "val dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9635  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9636  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9637  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9638  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9639  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9635  CAM_FRONT_RIGHT  \n",
            "9636  CAM_FRONT_RIGHT  \n",
            "9637  CAM_FRONT_RIGHT  \n",
            "9638  CAM_FRONT_RIGHT  \n",
            "9639  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9640 rows x 3 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type             | Params | Mode \n",
            "------------------------------------------------------\n",
            "0 | loss_fct | CrossEntropyLoss | 0      | train\n",
            "1 | input    | Sequential       | 224    | train\n",
            "2 | conv1    | Sequential       | 1.2 K  | train\n",
            "3 | conv2    | Sequential       | 4.6 K  | train\n",
            "4 | conv3    | Sequential       | 4.6 K  | train\n",
            "5 | conv4    | Sequential       | 1.2 K  | train\n",
            "6 | head     | Sequential       | 99     | train\n",
            "------------------------------------------------------\n",
            "11.9 K    Trainable params\n",
            "0         Non-trainable params\n",
            "11.9 K    Total params\n",
            "0.048     Total estimated model params size (MB)\n",
            "30        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89d1aef946cb4aff8da77c5355778d3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/home/mathis/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "  File \"/home/mathis/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "AttributeError: Can't get attribute 'ImageDataset' on <module '__main__' (built-in)>\n",
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:113\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start()\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:231\u001b[0m, in \u001b[0;36m_EvaluationLoop.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:104\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_PrefetchDataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;66;03m# ignore pre-fetching, it's not necessary\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/fetchers.py:51\u001b[0m, in \u001b[0;36m_DataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:351\u001b[0m, in \u001b[0;36mCombinedLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m iterator\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:155\u001b[0m, in \u001b[0;36m_Sequential.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_current_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/utilities/combined_loader.py:173\u001b[0m, in \u001b[0;36m_Sequential._load_current_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# No more iterables to step through, return an empty list\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/torch/utils/data/dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_epochs, accelerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# trainer = pl.Trainer(logger=logger, max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# start_t = time.perf_counter()\u001b[39;00m\n\u001b[1;32m     43\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, data_module)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f5efe7b9d30>> (for post_run_cell):\n"
          ]
        },
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/wandb_init.py:565\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/interface/interface.py:769\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py:289\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "# Init history memory\n",
        "history = history = {'train_loss':[],\n",
        "                        'val_loss':[],\n",
        "                        'test_loss':[],\n",
        "                        'train_accuracy':[],\n",
        "                        'val_accuracy':[],\n",
        "                        'test_accuracy':[],\n",
        "                        'test_results':[]}\n",
        "\n",
        "args.lr = 1e-3\n",
        "args.n_epochs = 20\n",
        "args.batch_size = 32\n",
        "args.conv_k = 3\n",
        "args.dropout = 0.2\n",
        "\n",
        "wandb.init(project='CameraNR',config=vars(args))\n",
        "\n",
        "# Load data\n",
        "data_module = DataModule(args=args, sensor='CAM',batch_size=args.batch_size,n_workers=args.n_workers)    \n",
        "# Loading labels\n",
        "labels, n_labels = get_labels(data_module)\n",
        "# init model\n",
        "model = CameraNDet(image_shape=[900,1600,3], output_size=n_labels, history=history, conv_k=args.conv_k, dropout_prob=args.dropout, lr=args.lr).to(device)\n",
        "model_ckpt_filename = 'camera_model.pth'\n",
        "hist_filename = 'camera_model_hist.pkl'\n",
        "# logger = WandbLogger(project=\"CameraNR\", name='EXP0')\n",
        "\n",
        "\n",
        "if args.load_checkpoint:\n",
        "    model.load_state_dict(torch.load(os.path.join(args.output_path, model_ckpt_filename)))\n",
        "    with open(os.path.join(args.output_path,hist_filename), 'rb') as f:\n",
        "        model.history = pickle.load(f)\n",
        "\n",
        "\n",
        "# init model\n",
        "trainer = pl.Trainer(max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\n",
        "# trainer = pl.Trainer(logger=logger, max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# training\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "# start_t = time.perf_counter()\n",
        "trainer.test(model, data_module)\n",
        "# end_t = time.perf_counter()\n",
        "# print('evaluated %d point clouds in %f seconds'%(len(data_module.df_test),end_t-start_t))\n",
        "\n",
        "# save hist (with test output)\n",
        "with open(os.path.join(args.output_path, hist_filename), 'wb') as f:\n",
        "    pickle.dump(history,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f284c0a",
      "metadata": {},
      "source": [
        "#### Radar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a7cb200",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init history memory\n",
        "history = history = {'train_loss':[],\n",
        "                        'val_loss':[],\n",
        "                        'test_loss':[],\n",
        "                        'train_accuracy':[],\n",
        "                        'val_accuracy':[],\n",
        "                        'test_accuracy':[],\n",
        "                        'test_results':[]}\n",
        "\n",
        "args.lr = 1e-4\n",
        "args.n_epochs = 50\n",
        "args.batch_size = 32\n",
        "args.dropout = 0.4\n",
        "\n",
        "if args.n_workers>0:\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "# Load data\n",
        "data_module = DataModule(args=args, sensor='RADAR',batch_size=args.batch_size,n_workers=args.n_workers)\n",
        "# Loading labels\n",
        "labels, n_labels = get_labels(data_module)\n",
        "# init model\n",
        "model = RadarNDet(n_channels=18, output_size=n_labels, history=history, dropout_prob=args.dropout, lr=args.lr).to(device)\n",
        "model_ckpt_filename = 'radar_model.pth'\n",
        "hist_filename = 'radar_model_hist.pkl'\n",
        "# logger = TensorBoardLogger('lightning_logs', name='radar_noise_classifier')\n",
        "# logger = WandbLogger(project=\"RadarNR\", name='EXP0')\n",
        "\n",
        "batch = next(iter(data_module.train_dataloader()))\n",
        "input_shape = batch[0].shape\n",
        "\n",
        "# print(batch)\n",
        "# print(input_shape)\n",
        "# raise KeyboardInterrupt\n",
        "\n",
        "\n",
        "\n",
        "if args.load_checkpoint:\n",
        "    model.load_state_dict(torch.load(os.path.join(args.output_path, model_ckpt_filename)))\n",
        "    with open(os.path.join(args.output_path,hist_filename), 'rb') as f:\n",
        "        model.history = pickle.load(f)\n",
        "\n",
        "\n",
        "wandb.init(project='RadarNR',config=vars(args))\n",
        "    \n",
        "# init model\n",
        "trainer = pl.Trainer(max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\n",
        "# trainer = pl.Trainer(logger=logger, max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# training\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "# save hist\n",
        "with open(os.path.join(args.output_path, hist_filename), 'wb') as f:\n",
        "    pickle.dump(history,f)\n",
        "\n",
        "# save trained weights\n",
        "if args.save_model:\n",
        "    torch.save(model.state_dict(), os.path.join(args.output_path, model_ckpt_filename))\n",
        "\n",
        "# testing \n",
        "# start_t = time.perf_counter()\n",
        "trainer.test(model, data_module)\n",
        "# end_t = time.perf_counter()\n",
        "# print('evaluated %d point clouds in %f seconds'%(len(data_module.df_test),end_t-start_t))\n",
        "\n",
        "# save hist (with test output)\n",
        "with open(os.path.join(args.output_path, hist_filename), 'wb') as f:\n",
        "    pickle.dump(history,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f2fd29bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======\n",
            "Loading NuScenes tables for version v1.0-mini...\n",
            "23 category,\n",
            "8 attribute,\n",
            "4 visibility,\n",
            "911 instance,\n",
            "12 sensor,\n",
            "120 calibrated_sensor,\n",
            "31206 ego_pose,\n",
            "8 log,\n",
            "10 scene,\n",
            "404 sample,\n",
            "31206 sample_data,\n",
            "18538 sample_annotation,\n",
            "4 map,\n",
            "Done loading in 0.701 seconds.\n",
            "======\n",
            "Reverse indexing ...\n",
            "Done reverse indexing in 0.1 seconds.\n",
            "======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type             | Params | Mode \n",
            "------------------------------------------------------\n",
            "0 | loss_fct | CrossEntropyLoss | 0      | train\n",
            "1 | input    | Sequential       | 224    | train\n",
            "2 | conv1    | Sequential       | 1.2 K  | train\n",
            "3 | conv2    | Sequential       | 4.6 K  | train\n",
            "4 | conv3    | Sequential       | 18.5 K | train\n",
            "5 | conv4    | Sequential       | 18.5 K | train\n",
            "6 | conv5    | Sequential       | 4.6 K  | train\n",
            "7 | conv6    | Sequential       | 1.2 K  | train\n",
            "8 | head     | Sequential       | 4.4 K  | train\n",
            "------------------------------------------------------\n",
            "53.2 K    Trainable params\n",
            "0         Non-trainable params\n",
            "53.2 K    Total params\n",
            "0.213     Total estimated model params size (MB)\n",
            "39        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sensor: CAM\n",
            "trainval: ['scene-0061', 'scene-0553', 'scene-0655', 'scene-0757', 'scene-0796']\n",
            "n_train_scenes: 1\n",
            "n_val_scenes: 1\n",
            "train_split: scene-0796\n",
            "val_split: scene-0553\n",
            "test_split: scene-0103\n",
            "train dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9635  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9636  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9637  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9638  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9639  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9635  CAM_FRONT_RIGHT  \n",
            "9636  CAM_FRONT_RIGHT  \n",
            "9637  CAM_FRONT_RIGHT  \n",
            "9638  CAM_FRONT_RIGHT  \n",
            "9639  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9640 rows x 3 columns]\n",
            "test dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9876  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9877  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9878  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9879  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9880  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9876  CAM_FRONT_RIGHT  \n",
            "9877  CAM_FRONT_RIGHT  \n",
            "9878  CAM_FRONT_RIGHT  \n",
            "9879  CAM_FRONT_RIGHT  \n",
            "9880  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9881 rows x 3 columns]\n",
            "val dataset:                                                    data  labels  \\\n",
            "0     ./data/noisy_nuScenes/samples/CAM_BACK/10/Blur...       1   \n",
            "1     ./data/noisy_nuScenes/samples/CAM_BACK/10/Gaus...       1   \n",
            "2     ./data/noisy_nuScenes/samples/CAM_BACK/10/High...       1   \n",
            "3     ./data/noisy_nuScenes/samples/CAM_BACK/10/Low_...       1   \n",
            "4     ./data/noisy_nuScenes/samples/CAM_BACK/20/Blur...       2   \n",
            "...                                                 ...     ...   \n",
            "9635  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9636  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9637  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9638  ./data/noisy_nuScenes/samples/CAM_FRONT_RIGHT/...      10   \n",
            "9639  ./data/default_nuScenes/samples/CAM_FRONT_RIGH...       0   \n",
            "\n",
            "               sensor  \n",
            "0            CAM_BACK  \n",
            "1            CAM_BACK  \n",
            "2            CAM_BACK  \n",
            "3            CAM_BACK  \n",
            "4            CAM_BACK  \n",
            "...               ...  \n",
            "9635  CAM_FRONT_RIGHT  \n",
            "9636  CAM_FRONT_RIGHT  \n",
            "9637  CAM_FRONT_RIGHT  \n",
            "9638  CAM_FRONT_RIGHT  \n",
            "9639  CAM_FRONT_RIGHT  \n",
            "\n",
            "[9640 rows x 3 columns]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfd8781f6fd44dd1925c1a70e27bc207",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mathis/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m legacycam2(image_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m900\u001b[39m,\u001b[38;5;241m1600\u001b[39m,\u001b[38;5;241m3\u001b[39m], output_size\u001b[38;5;241m=\u001b[39mn_labels, history\u001b[38;5;241m=\u001b[39mhistory, conv_k\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mconv_k, dropout_prob\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdropout, lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mn_epochs, accelerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[25], line 130\u001b[0m, in \u001b[0;36mlegacycam2.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> 130\u001b[0m     x, y, mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    131\u001b[0m     x, y, mask \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    132\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, mask\u001b[38;5;241m=\u001b[39mmask)\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "history = history = {'train_loss':[],\n",
        "                        'val_loss':[],\n",
        "                        'test_loss':[],\n",
        "                        'train_accuracy':[],\n",
        "                        'val_accuracy':[],\n",
        "                        'test_accuracy':[],\n",
        "                        'test_results':[]}\n",
        "\n",
        "args.lr = 1e-3\n",
        "args.n_epochs = 20\n",
        "args.batch_size = 32\n",
        "args.conv_k = 3\n",
        "args.dropout = 0.2\n",
        "\n",
        "# Load data\n",
        "data_module = DataModule(args=args, sensor='CAM',batch_size=args.batch_size,n_workers=args.n_workers)    \n",
        "# Loading labels\n",
        "labels, n_labels = get_labels(data_module)\n",
        "# init model\n",
        "model = legacycam2(image_shape=[900,1600,3], output_size=n_labels, history=history, conv_k=args.conv_k, dropout_prob=args.dropout, lr=args.lr).to(device)\n",
        "trainer = pl.Trainer(max_epochs=args.n_epochs, accelerator = 'gpu' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "trainer.fit(model, data_module)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddb54319",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type             | Params | Mode \n",
            "------------------------------------------------------\n",
            "0 | loss_fct | CrossEntropyLoss | 0      | train\n",
            "1 | input    | Sequential       | 608    | train\n",
            "2 | conv1    | Sequential       | 2.1 K  | train\n",
            "3 | conv2    | Sequential       | 8.3 K  | train\n",
            "4 | fc1      | Sequential       | 8.3 K  | train\n",
            "5 | fc2      | Sequential       | 2.1 K  | train\n",
            "6 | head     | Sequential       | 363    | train\n",
            "------------------------------------------------------\n",
            "21.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "21.7 K    Total params\n",
            "0.087     Total estimated model params size (MB)\n",
            "23        Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af50432fbbed441d818986beb4b095de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mathis/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "forward() got an unexpected keyword argument 'mask'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1023\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1049\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 105\u001b[0m, in \u001b[0;36mlegacyradar1.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m x, y, mask \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    104\u001b[0m x, y, mask \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 105\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#compute loss         \u001b[39;00m\n\u001b[1;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fct(y_hat, y)  \n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/synthcrav/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'mask'"
          ]
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "buk5w_lGCqo7",
        "srXjIkTFBbWZ",
        "2vSgdTOoFI9e",
        "iI-CmJsPJ1K2",
        "ZATXfRlsH7U2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "synthcrav",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
